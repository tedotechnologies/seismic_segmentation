{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# from sam2.build_sam import build_sam2\n",
    "\n",
    "sys.path.append(os.path.abspath('/home/dmatveev/workdir/rosneft_segmentation/experiments'))\n",
    "\n",
    "\n",
    "from prepare_data import SeismicDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_2d = {\n",
    "        \"type\": \"2D\",\n",
    "        \"seismic_dir\": \"/home/dmatveev/workdir/rosneft_segmentation/data/Salt2d/seismic\",\n",
    "        \"label_dir\": \"/home/dmatveev/workdir/rosneft_segmentation/data/Salt2d/label\",\n",
    "        \"shape\": (224, 224),\n",
    "        \"mask_dtype\": np.uint8,\n",
    "        \"augmentation_pipeline\": None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seismic_dataset = SeismicDataset(config_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(10):\n",
    "#     sample = seismic_dataset[idx]\n",
    "\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "#     axes[0].imshow(sample[\"seismic_img\"][:,:,0],cmap='gray')\n",
    "#     axes[0].axis(\"off\")\n",
    "\n",
    "#     axes[1].imshow(sample[\"label\"], cmap=\"gray\")\n",
    "#     axes[1].axis(\"off\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "    sample = self.dataset[idx]\n",
    "    # Convert image (HWC) to tensor (CHW)\n",
    "    seismic_img = torch.from_numpy(sample[\"seismic_img\"]).float().permute(2, 0, 1)\n",
    "    label = torch.from_numpy(sample[\"label\"]).long()\n",
    "    out = {\"seismic_img\": seismic_img, \"label\": label}\n",
    "    \n",
    "    # Always add mask_prompt if exists or set to None\n",
    "    if \"mask_prompt\" in sample:\n",
    "        if sample[\"mask_prompt\"] is not None:\n",
    "            out[\"mask_prompt\"] = torch.from_numpy(sample[\"mask_prompt\"]).long()\n",
    "        else:\n",
    "            out[\"mask_prompt\"] = None\n",
    "\n",
    "    # Always add point_prompt, even if it's None\n",
    "    point_prompt = sample.get(\"point_prompt\")\n",
    "    if point_prompt is not None:\n",
    "        out[\"point_prompt\"] = torch.from_numpy(point_prompt).float()\n",
    "    else:\n",
    "        out[\"point_prompt\"] = None\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = {\n",
    "    \"model\": {\n",
    "         \"checkpoint\": \"/home/dmatveev/workdir/rosneft_segmentation/models/sam2.1_hiera_base_plus.pt\",\n",
    "         \"config\": \"configs/sam2.1/sam2.1_hiera_b+.yaml\",\n",
    "         \"device\": \"cuda\",\n",
    "         \"use_ia3_adapters\": True,\n",
    "         \"ia3_adapter_size\": 64,  # example adapter size parameter\n",
    "         \"freeze_base\": True,\n",
    "    },\n",
    "    \"loss\": {\n",
    "         \"type\": \"CrossEntropyLoss\",  # or \"MSELoss\", etc.\n",
    "         \"params\": {}\n",
    "    },\n",
    "    \"training\": {\n",
    "         \"epochs\": 10,\n",
    "         \"batch_size\": 4,\n",
    "         \"lr\": 1e-4,\n",
    "         \"use_mask\": True,  # whether to use mask prompts during training\n",
    "         \"num_workers\": 4,\n",
    "         \"log_interval\": 10,\n",
    "    },\n",
    "    \"clearml\": {\n",
    "         \"project_name\": \"SAM2 Fine Tuning\",\n",
    "         \"task_name\": \"IA3 Adapter Training\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam2_checkpoint = train_config[\"model\"][\"checkpoint\"]\n",
    "model_cfg = train_config[\"model\"][\"config\"]\n",
    "device = train_config[\"model\"][\"device\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAM2Base(\n",
       "  (image_encoder): ImageEncoder(\n",
       "    (trunk): Hiera(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 112, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x MultiScaleBlock(\n",
       "          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (qkv): Linear(in_features=112, out_features=336, bias=True)\n",
       "            (proj): Linear(in_features=112, out_features=112, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=112, out_features=448, bias=True)\n",
       "              (1): Linear(in_features=448, out_features=112, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (2): MultiScaleBlock(\n",
       "          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
       "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "            (qkv): Linear(in_features=112, out_features=672, bias=True)\n",
       "            (proj): Linear(in_features=224, out_features=224, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=224, out_features=896, bias=True)\n",
       "              (1): Linear(in_features=896, out_features=224, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "          (proj): Linear(in_features=112, out_features=224, bias=True)\n",
       "        )\n",
       "        (3-4): 2 x MultiScaleBlock(\n",
       "          (norm1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (qkv): Linear(in_features=224, out_features=672, bias=True)\n",
       "            (proj): Linear(in_features=224, out_features=224, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=224, out_features=896, bias=True)\n",
       "              (1): Linear(in_features=896, out_features=224, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (5): MultiScaleBlock(\n",
       "          (norm1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
       "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "            (qkv): Linear(in_features=224, out_features=1344, bias=True)\n",
       "            (proj): Linear(in_features=448, out_features=448, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=448, out_features=1792, bias=True)\n",
       "              (1): Linear(in_features=1792, out_features=448, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "          (proj): Linear(in_features=224, out_features=448, bias=True)\n",
       "        )\n",
       "        (6-20): 15 x MultiScaleBlock(\n",
       "          (norm1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (qkv): Linear(in_features=448, out_features=1344, bias=True)\n",
       "            (proj): Linear(in_features=448, out_features=448, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=448, out_features=1792, bias=True)\n",
       "              (1): Linear(in_features=1792, out_features=448, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (21): MultiScaleBlock(\n",
       "          (norm1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
       "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "            (qkv): Linear(in_features=448, out_features=2688, bias=True)\n",
       "            (proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((896,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=896, out_features=3584, bias=True)\n",
       "              (1): Linear(in_features=3584, out_features=896, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "          (proj): Linear(in_features=448, out_features=896, bias=True)\n",
       "        )\n",
       "        (22-23): 2 x MultiScaleBlock(\n",
       "          (norm1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (qkv): Linear(in_features=896, out_features=2688, bias=True)\n",
       "            (proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((896,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=896, out_features=3584, bias=True)\n",
       "              (1): Linear(in_features=3584, out_features=896, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): FpnNeck(\n",
       "      (position_encoding): PositionEmbeddingSine()\n",
       "      (convs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (conv): Conv2d(448, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (conv): Conv2d(224, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (conv): Conv2d(112, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mask_downsample): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))\n",
       "  (memory_attention): MemoryAttention(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x MemoryAttentionLayer(\n",
       "        (self_attn): RoPEAttention(\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (cross_attn_image): RoPEAttention(\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (memory_encoder): MemoryEncoder(\n",
       "    (mask_downsampler): MaskDownSampler(\n",
       "      (encoder): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (7): LayerNorm2d()\n",
       "        (8): GELU(approximate='none')\n",
       "        (9): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (10): LayerNorm2d()\n",
       "        (11): GELU(approximate='none')\n",
       "        (12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (pix_feat_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fuser): Fuser(\n",
       "      (proj): Identity()\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x CXBlock(\n",
       "          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "          (norm): LayerNorm2d()\n",
       "          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (position_encoding): PositionEmbeddingSine()\n",
       "    (out_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (sam_prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (sam_mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (1): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            )\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (obj_score_token): Embedding(1, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (conv_s0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv_s1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "        (act): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (pred_obj_score_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (act): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (obj_ptr_proj): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (act): ReLU()\n",
       "  )\n",
       "  (obj_ptr_tpos_proj): Linear(in_features=256, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\n",
    "sam2_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_config[\"model\"][\"freeze_base\"]:\n",
    "    for param in sam2_model.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IA3Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Оборачивает существующий линейный слой (nn.Linear) и добавляет к его выходу\n",
    "    поэлементное масштабирование. Исходный линейный слой замораживается.\n",
    "    \"\"\"\n",
    "    def __init__(self, module: nn.Linear):\n",
    "        super().__init__()\n",
    "        self.module = module  # предобученный (замороженный) слой\n",
    "        # Инициализируем масштабный вектор единицами (по числу выходных признаков)\n",
    "        self.scale = nn.Parameter(torch.ones(module.out_features, device=module.weight.device))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Вычисляем обычный выход слоя\n",
    "        out = self.module(input)\n",
    "        # Применяем поэлементное масштабирование: каждый выходной канал умножается на свой scale\n",
    "        return out * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\n",
    "sam2_model.to(device)\n",
    "\n",
    "# Замораживаем все параметры базовой модели\n",
    "for param in sam2_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ia3 = 0\n",
    "for block in sam2_model.image_encoder.trunk.blocks:\n",
    "    # Если блок имеет модуль внимания, оборачиваем его проекции\n",
    "    if hasattr(block, \"attn\"):\n",
    "        # Оборачиваем линейный слой qkv\n",
    "        if hasattr(block.attn, \"qkv\") and isinstance(block.attn.qkv, nn.Linear):\n",
    "            block.attn.qkv = IA3Layer(block.attn.qkv).to(device)\n",
    "            num_ia3 += 1\n",
    "        # Оборачиваем линейный слой proj\n",
    "        if hasattr(block.attn, \"proj\") and isinstance(block.attn.proj, nn.Linear):\n",
    "            block.attn.proj = IA3Layer(block.attn.proj).to(device)\n",
    "            num_ia3 += 1\n",
    "    # Если блок содержит MLP, оборачиваем каждый линейный слой внутри него\n",
    "    if hasattr(block, \"mlp\") and hasattr(block.mlp, \"layers\"):\n",
    "        for i, layer in enumerate(block.mlp.layers):\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                block.mlp.layers[i] = IA3Layer(layer).to(device)\n",
    "                num_ia3 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of IA³ adapters inserted: 96\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of IA³ adapters inserted:\", num_ia3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# После интеграции оставляем для обучения только параметры адаптеров (scale)\n",
    "for name, param in sam2_model.named_parameters():\n",
    "    if \"scale\" not in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучаемых параметров: 96768\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in sam2_model.parameters() if p.requires_grad)\n",
    "print(\"Обучаемых параметров:\", trainable_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_encoder.trunk.blocks.0.attn.qkv.scale: 336 параметров\n",
      "image_encoder.trunk.blocks.0.attn.proj.scale: 112 параметров\n",
      "image_encoder.trunk.blocks.0.mlp.layers.0.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.0.mlp.layers.1.scale: 112 параметров\n",
      "image_encoder.trunk.blocks.1.attn.qkv.scale: 336 параметров\n",
      "image_encoder.trunk.blocks.1.attn.proj.scale: 112 параметров\n",
      "image_encoder.trunk.blocks.1.mlp.layers.0.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.1.mlp.layers.1.scale: 112 параметров\n",
      "image_encoder.trunk.blocks.2.attn.qkv.scale: 672 параметров\n",
      "image_encoder.trunk.blocks.2.attn.proj.scale: 224 параметров\n",
      "image_encoder.trunk.blocks.2.mlp.layers.0.scale: 896 параметров\n",
      "image_encoder.trunk.blocks.2.mlp.layers.1.scale: 224 параметров\n",
      "image_encoder.trunk.blocks.3.attn.qkv.scale: 672 параметров\n",
      "image_encoder.trunk.blocks.3.attn.proj.scale: 224 параметров\n",
      "image_encoder.trunk.blocks.3.mlp.layers.0.scale: 896 параметров\n",
      "image_encoder.trunk.blocks.3.mlp.layers.1.scale: 224 параметров\n",
      "image_encoder.trunk.blocks.4.attn.qkv.scale: 672 параметров\n",
      "image_encoder.trunk.blocks.4.attn.proj.scale: 224 параметров\n",
      "image_encoder.trunk.blocks.4.mlp.layers.0.scale: 896 параметров\n",
      "image_encoder.trunk.blocks.4.mlp.layers.1.scale: 224 параметров\n",
      "image_encoder.trunk.blocks.5.attn.qkv.scale: 1344 параметров\n",
      "image_encoder.trunk.blocks.5.attn.proj.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.5.mlp.layers.0.scale: 1792 параметров\n",
      "image_encoder.trunk.blocks.5.mlp.layers.1.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.6.attn.qkv.scale: 1344 параметров\n",
      "image_encoder.trunk.blocks.6.attn.proj.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.6.mlp.layers.0.scale: 1792 параметров\n",
      "image_encoder.trunk.blocks.6.mlp.layers.1.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.7.attn.qkv.scale: 1344 параметров\n",
      "image_encoder.trunk.blocks.7.attn.proj.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.7.mlp.layers.0.scale: 1792 параметров\n",
      "image_encoder.trunk.blocks.7.mlp.layers.1.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.8.attn.qkv.scale: 1344 параметров\n",
      "image_encoder.trunk.blocks.8.attn.proj.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.8.mlp.layers.0.scale: 1792 параметров\n",
      "image_encoder.trunk.blocks.8.mlp.layers.1.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.9.attn.qkv.scale: 1344 параметров\n",
      "image_encoder.trunk.blocks.9.attn.proj.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.9.mlp.layers.0.scale: 1792 параметров\n",
      "image_encoder.trunk.blocks.9.mlp.layers.1.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.10.attn.qkv.scale: 1344 параметров\n",
      "image_encoder.trunk.blocks.10.attn.proj.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.10.mlp.layers.0.scale: 1792 параметров\n",
      "image_encoder.trunk.blocks.10.mlp.layers.1.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.11.attn.qkv.scale: 1344 параметров\n",
      "image_encoder.trunk.blocks.11.attn.proj.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.11.mlp.layers.0.scale: 1792 параметров\n",
      "image_encoder.trunk.blocks.11.mlp.layers.1.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.12.attn.qkv.scale: 1344 параметров\n",
      "image_encoder.trunk.blocks.12.attn.proj.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.12.mlp.layers.0.scale: 1792 параметров\n",
      "image_encoder.trunk.blocks.12.mlp.layers.1.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.13.attn.qkv.scale: 1344 параметров\n",
      "image_encoder.trunk.blocks.13.attn.proj.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.13.mlp.layers.0.scale: 1792 параметров\n",
      "image_encoder.trunk.blocks.13.mlp.layers.1.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.14.attn.qkv.scale: 1344 параметров\n",
      "image_encoder.trunk.blocks.14.attn.proj.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.14.mlp.layers.0.scale: 1792 параметров\n",
      "image_encoder.trunk.blocks.14.mlp.layers.1.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.15.attn.qkv.scale: 1344 параметров\n",
      "image_encoder.trunk.blocks.15.attn.proj.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.15.mlp.layers.0.scale: 1792 параметров\n",
      "image_encoder.trunk.blocks.15.mlp.layers.1.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.16.attn.qkv.scale: 1344 параметров\n",
      "image_encoder.trunk.blocks.16.attn.proj.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.16.mlp.layers.0.scale: 1792 параметров\n",
      "image_encoder.trunk.blocks.16.mlp.layers.1.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.17.attn.qkv.scale: 1344 параметров\n",
      "image_encoder.trunk.blocks.17.attn.proj.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.17.mlp.layers.0.scale: 1792 параметров\n",
      "image_encoder.trunk.blocks.17.mlp.layers.1.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.18.attn.qkv.scale: 1344 параметров\n",
      "image_encoder.trunk.blocks.18.attn.proj.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.18.mlp.layers.0.scale: 1792 параметров\n",
      "image_encoder.trunk.blocks.18.mlp.layers.1.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.19.attn.qkv.scale: 1344 параметров\n",
      "image_encoder.trunk.blocks.19.attn.proj.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.19.mlp.layers.0.scale: 1792 параметров\n",
      "image_encoder.trunk.blocks.19.mlp.layers.1.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.20.attn.qkv.scale: 1344 параметров\n",
      "image_encoder.trunk.blocks.20.attn.proj.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.20.mlp.layers.0.scale: 1792 параметров\n",
      "image_encoder.trunk.blocks.20.mlp.layers.1.scale: 448 параметров\n",
      "image_encoder.trunk.blocks.21.attn.qkv.scale: 2688 параметров\n",
      "image_encoder.trunk.blocks.21.attn.proj.scale: 896 параметров\n",
      "image_encoder.trunk.blocks.21.mlp.layers.0.scale: 3584 параметров\n",
      "image_encoder.trunk.blocks.21.mlp.layers.1.scale: 896 параметров\n",
      "image_encoder.trunk.blocks.22.attn.qkv.scale: 2688 параметров\n",
      "image_encoder.trunk.blocks.22.attn.proj.scale: 896 параметров\n",
      "image_encoder.trunk.blocks.22.mlp.layers.0.scale: 3584 параметров\n",
      "image_encoder.trunk.blocks.22.mlp.layers.1.scale: 896 параметров\n",
      "image_encoder.trunk.blocks.23.attn.qkv.scale: 2688 параметров\n",
      "image_encoder.trunk.blocks.23.attn.proj.scale: 896 параметров\n",
      "image_encoder.trunk.blocks.23.mlp.layers.0.scale: 3584 параметров\n",
      "image_encoder.trunk.blocks.23.mlp.layers.1.scale: 896 параметров\n"
     ]
    }
   ],
   "source": [
    "for name, param in sam2_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.numel()} параметров\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = train_config[\"loss\"][\"type\"]\n",
    "if loss_type == \"CrossEntropyLoss\":\n",
    "    criterion = nn.CrossEntropyLoss(**train_config[\"loss\"][\"params\"])\n",
    "elif loss_type == \"MSELoss\":\n",
    "    criterion = nn.MSELoss(**train_config[\"loss\"][\"params\"])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported loss type: {loss_type}\")\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, sam2_model.parameters()),\n",
    "                       lr=train_config[\"training\"][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset = TorchSeismicDataset(seismic_dataset)\n",
    "\n",
    "train_loader = DataLoader(torch_dataset,\n",
    "                          batch_size=train_config[\"training\"][\"batch_size\"],\n",
    "                          shuffle=True,\n",
    "                          num_workers=train_config[\"training\"][\"num_workers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f085d913bd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = train_config[\"training\"][\"epochs\"]\n",
    "log_interval = train_config[\"training\"][\"log_interval\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Please use the corresponding methods in SAM2VideoPredictor for inference or SAM2Train for training/fine-tuningSee notebooks/video_predictor_example.ipynb for an inference example.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     mask_prompts \u001b[38;5;241m=\u001b[39m mask_prompts\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Forward pass: adjust arguments as per your model's API\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43msam2_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseismic_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoint_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoint_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_prompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Compute loss (assuming outputs shape is [B, num_classes, H, W])\u001b[39;00m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/workdir/rosneft_segmentation/for_rosneft/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workdir/rosneft_segmentation/for_rosneft/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/workdir/rosneft_segmentation/sam2/sam2/modeling/sam2_base.py:202\u001b[0m, in \u001b[0;36mSAM2Base.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use the corresponding methods in SAM2VideoPredictor for inference or SAM2Train for training/fine-tuning\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee notebooks/video_predictor_example.ipynb for an inference example.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Please use the corresponding methods in SAM2VideoPredictor for inference or SAM2Train for training/fine-tuningSee notebooks/video_predictor_example.ipynb for an inference example."
     ]
    }
   ],
   "source": [
    "sam2_model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move data to the specified device\n",
    "        seismic_imgs = batch[\"seismic_img\"].to(device)   # [B, C, H, W]\n",
    "        labels = batch[\"label\"].to(device)                 # [B, H, W]\n",
    "        \n",
    "        # Optionally pass prompts if available and enabled\n",
    "        point_prompts = batch.get(\"point_prompt\")\n",
    "        if point_prompts is not None:\n",
    "            point_prompts = point_prompts.to(device)\n",
    "        mask_prompts = batch.get(\"mask_prompt\")\n",
    "        if mask_prompts is not None and train_config[\"training\"][\"use_mask\"]:\n",
    "            mask_prompts = mask_prompts.to(device)\n",
    "        \n",
    "        # Forward pass: adjust arguments as per your model's API\n",
    "        outputs = sam2_model(seismic_imgs, point_prompt=point_prompts, mask_prompt=mask_prompts)\n",
    "        \n",
    "        # Compute loss (assuming outputs shape is [B, num_classes, H, W])\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            current_iter = epoch * len(train_loader) + batch_idx\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{batch_idx}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n",
    "            # task.get_logger().report_scalar(\"loss\", \"train\", iteration=current_iter, value=loss.item())\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_loss:.4f}\")\n",
    "    # task.get_logger().report_scalar(\"epoch_loss\", \"train\", iteration=epoch, value=avg_loss)\n",
    "    \n",
    "    # Save checkpoint for the epoch\n",
    "    checkpoint_path = f\"checkpoint_epoch_{epoch+1}.pt\"\n",
    "    torch.save({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"model_state_dict\": sam2_model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": avg_loss,\n",
    "    }, checkpoint_path)\n",
    "    # task.get_logger().report_artifact(name=f\"checkpoint_epoch_{epoch+1}\", artifact_object=checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_rosneft",
   "language": "python",
   "name": "for_rosneft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
